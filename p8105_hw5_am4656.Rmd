---
title: "p8105_hw5_am4656"
author: "Aaron Mittel"
date: "2022-11-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1)
```

# Problem 1
In this problem, we create a dataframe containing data from participants in the downloaded "data" folder. Each participant's data is saved in a separate file within this folder, which can be mapped in iterative fashion using the read_csv function.
```{r creating study data dataframe from files in data folder, message = FALSE, include = TRUE, echo = FALSE}
file_names =
list.files(path = "./data") %>% 
  str_replace("con","data/con") %>% 
  str_replace("exp","data/exp") %>% 
  as_tibble()

study_data = map_dfr(file_names, read_csv) %>% 
  mutate(
    id = row_number(),
    arm = ifelse(id < 11, "control","experimental"),
  ) %>% 
  relocate(id,arm) %>% 
  pivot_longer(
    week_1:week_8,
    values_to = "value",
    names_to = "week") %>% 
  mutate(
    week = (str_replace(week, "week_","")),
    week = as.numeric(week)) %>% 
  group_by(week)

study_data %>% 
  ggplot(aes(x = week, y = value, color = factor(id))) +
  geom_line() + geom_point() +
  facet_grid(~arm) +
  viridis::scale_color_viridis(discrete = TRUE)
```

In general, patients in the experimental group have values that increase over time. Patients in the control group have values that remain relatively stable over time.

# Problem 2
```{r creating homicide dataframe, message = FALSE, include = FALSE}
homicide_data = 
  read_csv("./homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state))
```

This dataframe contains 12 columns which report location, assigned case id ('uid'), date of homicide,  victim demographics, and case outcome (`disposition`) of homicides from the decade preceding 2018. There are `r nrow(homicide_data)` rows and `r homicide_data %>% select(city) %>% distinct %>% nrow` distinct cities. Cities with missing data were removed.

```{r creating homicide_data dataframe focusing on solved vs unsolved homicides, include = FALSE, message = FALSE}
homicide_data_solved =
  homicide_data %>%
  group_by(city_state, disposition) %>% 
  filter(
    disposition == "Closed by arrest" | disposition == "Open/No arrest" | disposition == "Closed without arrest") %>% 
  summarize(
    number = n()) %>% 
  pivot_wider(
    names_from = disposition,
    values_from = number
  ) %>% 
  rowwise() %>% 
  mutate(
    unsolved = sum(c_across("Closed without arrest":"Open/No arrest")),
    total = sum(c_across("Closed by arrest":"Open/No arrest")),
    proportion_unsolved = unsolved/total,
    unsolved = as.numeric(unsolved),
    total = as.numeric(total)) %>% 
  drop_na() %>% 
  ungroup()
```

## Estimating the Proportion of Unsolved Homicides in Baltimore, MD
```{r baltimore-focused solved vs unsolved dataframe, include = TRUE, echo = FALSE, message = FALSE}
homicide_data_solved_baltimore =
homicide_data_solved %>% 
  filter(
    city_state == "Baltimore, MD")

prop.test(x = homicide_data_solved_baltimore %>% pull(unsolved), n = homicide_data_solved_baltimore %>% pull(total), alternative = c("two.sided"), conf.level = 0.95) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 2, caption = "Table 1. Estimate and 95% CI for proportion of murders that are unsolved in Baltimore, MD")
```

## Estimating the Proportion of Unsolved Homicides in All Cities in Database
```{r all city focused estimates of unsolved homicides, include = TRUE, echo = FALSE, message = FALSE}
homicide_data_solved_nest =
  nest(homicide_data_solved, data = "Closed by arrest":"proportion_unsolved")
```

```{r creating a function for 1-sample proportion test of each dataframe and testing it on a single city_state, include = TRUE, echo = FALSE, message = FALSE}
prop_test_homicide = function(df) {
  prop.test(df$unsolved, df$total) %>% 
    broom::tidy()
}

prop_test_homicide(homicide_data_solved_nest$data[[35]])
```

```{r mapping this function to all city_states within the list of data and then creating a tidy data frame with variables of interest, include = TRUE, echo = FALSE, message = FALSE}
map(homicide_data_solved_nest$data, prop_test_homicide)

homicide_data_estimates =
  homicide_data_solved_nest %>% 
  mutate(
    estimates_df = map(homicide_data_solved_nest$data, prop_test_homicide)) %>% 
  unnest(estimates_df)

homicide_data_estimates_clean =
  homicide_data_estimates %>% 
  select(-data,-statistic,-parameter,-p.value,-method,-alternative)

homicide_data_estimates_clean
```

Create a plot that shows the estimates and CIs for each city â€“ check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.
```{r plot w estimates and CIs for each city}
estimates_plot = 
  ggplot(homicide_data_estimates_clean, aes(x = city_state, y = estimate)) +
  geom_point(aes(color = city_state))
```



# Problem 3
```{r mu = 0, include = TRUE, message = FALSE, echo = FALSE}
mean_ttest = function(n = 30, mu = 0, sigma = 5) {  
  prob3_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
    )
  
  prob3_data %>% 
    summarize(
      mu_sample = mean(x)
    )
}
  
  t_test = t.test(x, alternative = "two.sided", mu = 0, paired = FALSE, alpha = 0.05) %>%
        broom::tidy()
  
prob3_df_mean0 = 
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mu, mean_ttest)) %>% 
  unnest(estimate_df)
```

```{r, mu varies from 0 - 6, include = TRUE, message = FALSE, echo = FALSE}
prob3_df = 
  expand_grid(
    sample_size = 30,
    mean = c(0,1,2,3,4,5,6),
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mean, mean_ttest)) %>% 
  unnest(estimate_df)


prob3_df %>% 
  mutate(
    significant = if_else(t_test$p.value < 0.05, TRUE, FALSE)
  ) %>% 
  group_by(mean, significant) %>% 
  summarize(
    n = n())
```

y-axis should be power, which is = 1 - beta. Beta is the chance of making a type II error (i.e. false negative rate). Thus, y-axis should be the proportion of 

```{r}
sim_results_test = 
  expand_grid(
    mu = c(6),
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mu, sim_mean_sd)
  ) %>% 
  unnest(estimate_df)
```

