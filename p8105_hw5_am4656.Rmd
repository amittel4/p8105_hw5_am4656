---
title: "p8105_hw5_am4656"
author: "Aaron Mittel"
date: "2022-11-11"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1)
```

# Problem 1
In this problem, we created a dataframe containing data from participants in the downloaded "data" folder. Each participant's data is saved in a separate file within this folder, which can be mapped in iterative fashion using the read_csv function. A spaghetti plot shows observations on each subject over time.

```{r creating study data dataframe from files in data folder, message = FALSE, include = TRUE, echo = FALSE}
file_names =
list.files(path = "./data") %>% 
  str_replace("con","data/con") %>% 
  str_replace("exp","data/exp") %>% 
  as_tibble()

study_data = map_dfr(file_names, read_csv) %>% 
  mutate(
    id = row_number(),
    arm = ifelse(id < 11, "control","experimental"),
  ) %>% 
  relocate(id,arm) %>% 
  pivot_longer(
    week_1:week_8,
    values_to = "value",
    names_to = "week") %>% 
  mutate(
    week = (str_replace(week, "week_","")),
    week = as.numeric(week)) %>% 
  group_by(week)

study_data %>% 
  ggplot(aes(x = week, y = value, color = factor(id))) +
  geom_line() + geom_point() +
  facet_grid(~arm) +
  viridis::scale_color_viridis(discrete = TRUE) +
  labs(
    title = "Figure 1. Spaghetti plot of observations on each subject over time"
  )
```

In general, patients in the experimental group have values that increase over time. Patients in the control group have values that remain relatively stable over time.


# Problem 2
```{r creating homicide dataframe, message = FALSE, include = FALSE}
homicide_data = 
  read_csv("./homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, ", ", state))
```

This dataframe contains 12 columns which report location, assigned case id (`uid`), date of homicide,  victim demographics, and case outcome (`disposition`) of homicides from the decade preceding 2018. There are `r nrow(homicide_data)` rows and `r homicide_data %>% select(city) %>% distinct %>% nrow` distinct cities.

```{r creating homicide_data dataframe focusing on solved vs unsolved homicides, include = FALSE, message = FALSE}
homicide_data_solved =
  homicide_data %>%
  group_by(city_state, disposition) %>% 
  filter(
    disposition == "Closed by arrest" | disposition == "Open/No arrest" | disposition == "Closed without arrest") %>% 
  summarize(
    number = n()) %>% 
  pivot_wider(
    names_from = disposition,
    values_from = number
  ) %>% 
  rowwise() %>% 
  mutate(
    unsolved = sum(c_across("Closed without arrest":"Open/No arrest")),
    total = sum(c_across("Closed by arrest":"Open/No arrest")),
    proportion_unsolved = unsolved/total) %>% 
  drop_na() %>% 
  ungroup()
```

_Cities with missing data were removed._

## _Estimating the Proportion of Unsolved Homicides in Baltimore, MD_
Here, data is presented with the `estimate` of unsolved murders in Baltimore, MD as well as the associated 95% Confidence Interval (`conf.low` - `conf.high`).

```{r baltimore-focused solved vs unsolved dataframe, include = TRUE, echo = FALSE, message = FALSE}
homicide_data_solved_baltimore =
homicide_data_solved %>% 
  filter(
    city_state == "Baltimore, MD")

prop.test(x = homicide_data_solved_baltimore %>% pull(unsolved), n = homicide_data_solved_baltimore %>% pull(total), alternative = c("two.sided"), conf.level = 0.95) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 2, caption = "Table 1. Estimate and 95% CI for proportion of murders that are unsolved in Baltimore, MD")
```

## _Estimating the Proportion of Unsolved Homicides in All Cities in Database_
Here, all cities with data were included in the analysis. Data in the Table below represents point `estimate`s for the proportion of murders which went unsolved and the associated 95% Confidence Interval (`conf.low` - `conf.high`).
```{r all city focused estimates of unsolved homicides nested data, include = TRUE, echo = FALSE, message = FALSE}
homicide_data_solved_nest =
  nest(homicide_data_solved, data = "Closed by arrest":"proportion_unsolved")
```

```{r creating a function for 1-sample proportion test of each dataframe and testing it on a single city_state, include = FALSE, echo = FALSE, message = FALSE}
prop_test_homicide = function(df) {
  prop.test(df$unsolved, df$total) %>% 
    broom::tidy()
}

prop_test_homicide(homicide_data_solved_nest$data[[35]])
```

```{r mapping this function to all city_states within the list of data and then creating a tidy data frame with variables of interest, include = FALSE, echo = FALSE, message = FALSE}
map(homicide_data_solved_nest$data, prop_test_homicide)

homicide_data_estimates =
  homicide_data_solved_nest %>% 
  mutate(
    estimates_df = map(homicide_data_solved_nest$data, prop_test_homicide)) %>% 
  unnest(estimates_df)

homicide_data_estimates_clean =
  homicide_data_estimates %>% 
  select(-data,-statistic,-parameter,-p.value,-method,-alternative)
```

```{r presenting data as a kable, include = TRUE, echo = FALSE, message = FALSE}
homicide_data_estimates_clean %>% 
  knitr::kable(digits = 2, caption = "Table 2. Estimated proportion of unsolved murders (and 95% CI) in each city with data in the data set")
```


The plot in Figure 1, below, shows the estimates and CIs (y-axis) for each city (x-axis). Cities have been organized according to the proportion of unsolved homicides.
```{r plot w estimates and CIs for each city, include = TRUE, echo = FALSE, message = FALSE}
estimates_plot = 
  homicide_data_estimates_clean %>% 
  mutate(
    city_state = forcats::fct_reorder(city_state,estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate*100)) +
  geom_point() + geom_errorbar(aes(ymax = conf.high*100, ymin = conf.low*100)) +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Figure 2. Plot of Unsolved Murder Proportion by City, State",
    x = "",
    y = "Prop. Murders Unsolved",
    caption = "Data from 2008 - 2018"
  )

estimates_plot
```


# Problem 3

This problem uses simulation to explore how power of a 1-sample t-test changes as sample mean changes during sampling.

First, I generated 5000 datasets from a normal distribution of n = 30 subjects who have sigma = 5 and a mean of 0. I performed a simple t-test to determine if the sampled mean is significantly different than the true, set mean of 0 using a de novo function `sim_mean_ttest`. The figure indicates the outcome of 5000-fold iterative process.
```{r sampling around mean 0, include = TRUE, message = FALSE, echo = FALSE}
sim_mean_ttest = function(true_mean) {
  sim_data = tibble(
    x = rnorm(n = 30, mean = true_mean, sd = 5)
  )
  
  sim_data %>% 
    summarize(
      mu_hat = mean(x), 
      t_test = t.test(x, alternative = "two.sided", mu = 0, paired = FALSE, alpha = 0.05) %>%
        broom::tidy()
    )
}

sample_df_mean0 =
  expand.grid(
    true_mean = 0,
    iteration = 1:5000
  ) %>% 
  mutate(
    prob3_estimates_df = map(true_mean, sim_mean_ttest)
  ) %>% 
  unnest(prob3_estimates_df) %>% 
  unnest(t_test)

sample_df_mean0 %>% 
  ggplot(aes(x = mu_hat)) + 
  geom_density() + 
  labs(
    title = "Figure 3. Density of mu_hat indicating normal distribution; 5000 iterations",
    caption = "Constant: 30 subjects, standard deviation of 5"
  )

sample_df_mean0_tidy = 
sample_df_mean0 %>% 
  pivot_longer(
    mu_hat:conf.high,
    names_to = "parameter",
    values_to = "estimate") %>% 
  relocate(iteration) %>% 
  select(iteration, true_mean, parameter, estimate)
```

Now, repeating this process when sample mean varies from 1 - 6:
```{r sampling around mean 1 - 6, include = FALSE, message = FALSE, echo = FALSE}
sample_df_means = 
  expand.grid(
    true_mean = c(1,2,3,4,5,6),
    iteration = 1:5000
  ) %>% 
  mutate(
    prob3_b_estimates_df = map(true_mean, sim_mean_ttest)
  ) %>% 
  unnest(prob3_b_estimates_df) %>% 
  unnest(t_test)

sample_df_means
```

```{r multiple means density plot, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
sample_df_means_plot = 
  sample_df_means %>% 
  group_by(true_mean) %>% 
  ggplot(aes(x = mu_hat)) + 
  geom_density() + 
  facet_grid(. ~ true_mean) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(
    title = "Figure 4. Density of mu_hat indicating normal distribution, by true mean; 5000 iterations",
    caption = "Constant: 30 subjects, standard deviation of 5"    
  )

sample_df_means_plot
```

The null hypothesis for this t-test is that the difference between the sample mean and the true mean is 0. When p < 0.05, we reject the null.

```{r p rejection plot, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
rejection_plot = 
sample_df_means %>% 
  group_by(true_mean) %>% 
  mutate(
    significant = if_else(p.value < 0.05, "significant", "not_significant")
  ) %>% 
  select(true_mean, iteration, mu_hat, estimate, p.value, significant) %>% 
  group_by(true_mean, significant) %>% 
  summarize(
    n = n()) %>% 
  pivot_wider(
    names_from = significant,
    values_from = n
  ) %>% 
  replace(is.na(.),0) %>% 
  mutate(
    proportion_significant = (significant/(significant + not_significant))
  ) %>% 
  ggplot(aes(x = true_mean, y = proportion_significant)) +
  geom_point() + geom_smooth() +
  labs(
    title = "Figure 5. Proportion of times rejecting null; 5000 iterations",
    caption = "Null hypothesis: no difference between sample and true mean.",
    x = "True Mean",
    y = "Proportion of Times Rejecting Null Hypothesis (Power)"
  ) +
  theme_minimal()

rejection_plot
```

Below, I have made a plot showing the average estimate of sample me on the y axis and the true mean on the x axis. The smoothed line indicates comparison among all estimated means while the overlaid points indicate only means in which the null hypothesis was rejected.
```{r sample vs true mean plot setup, include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
mean_compare_data =
  sample_df_means %>% 
  group_by(true_mean) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  )

mean_significant_data = 
  sample_df_means %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(
    mean_mu_hat_signif = mean(mu_hat)
  )
```

```{r comparison plot, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
mean_comparison_plot = 
  ggplot(data = mean_compare_data, aes(x = true_mean, y = mean_mu_hat)) +
  geom_smooth() + 
  geom_point(data = mean_significant_data, aes(x = true_mean, y = mean_mu_hat_signif)) +
  labs(
    title = "Figure 6. Comparison of sample mean versus true mean among ",
    caption = "5000 iterations",
    x = "True Mean",
    y = "Average Estimate of Sample Mean"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

mean_comparison_plot
```

_The average of sample mean for which the null is rejected is approximately equal to the true value of the mean only when the true mean is larger than 3. This reflects the influence of the standard deviation of 5, leading to greater spread in data at smaller mean values._